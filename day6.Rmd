---
title: "Day 6"
author: "Vebash Naidoo"
date: "12/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Higher Dimensions
Given the table in the video above, what would the dimensions be for input features (x), the weights (W), and the bias (b) to satisfy (Wx + b)?

W: (1 x n), x: (n x 1), b: (1 x 1)

## Perceptrons as Logical Operators

### And Perceptron 
<img src = 'and.png'>

### Or Perceptron
<img src = 'or.png'>

The OR perceptron is very similar to an AND perceptron. In the image below, the OR perceptron has the same line as the AND perceptron, except the line is shifted down. What can you do to the weights and/or bias to achieve this? Use the following AND perceptron to create an OR Perceptron.

<img src = 'and_to_or.png'>

> What are two ways to go from an AND perceptron to an OR perceptron?
>
    > X Increase the weights
    >   Decrease the weights
    >   Increase a single weight
    >   Decrease a single weight
    >   Increase the magnotude of the bias
    > X Decrease the magnitude of the bias


### What are the weights and bias for the AND perceptron?
Set the weights (weight1, weight2) and bias (bias) to values that will correctly determine the AND operation as shown above.
More than one set of values will work!
```{python logic}
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 1.0
weight2 = 1.0
bias = -2.0


# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [False, False, False, True]
outputs = []

# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])

# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
```

### NOT Perceptron
Unlike the other perceptrons we looked at, the NOT operation only cares about one input. The operation returns a 0 if the input is 1 and a 1 if it's a 0. The other inputs to the perceptron are ignored.

In this quiz, you'll set the weights (weight1, weight2) and bias bias to the values that calculate the NOT operation on the second input and ignores the first input.

```{python logic2}
import pandas as pd

# TODO: Set weight1, weight2, and bias
weight1 = 0.0
weight2 = -1.0
bias = 0.0


# DON'T CHANGE ANYTHING BELOW
# Inputs and outputs
test_inputs = [(0, 0), (0, 1), (1, 0), (1, 1)]
correct_outputs = [True, False, True, False]
outputs = []

# Generate and check output
for test_input, correct_output in zip(test_inputs, correct_outputs):
    linear_combination = weight1 * test_input[0] + weight2 * test_input[1] + bias
    output = int(linear_combination >= 0)
    is_correct_string = 'Yes' if output == correct_output else 'No'
    outputs.append([test_input[0], test_input[1], linear_combination, output, is_correct_string])

# Print output
num_wrong = len([output[4] for output in outputs if output[4] == 'No'])
output_frame = pd.DataFrame(outputs, columns=['Input 1', '  Input 2', '  Linear Combination', '  Activation Output', '  Is Correct'])
if not num_wrong:
    print('Nice!  You got it all correct.\n')
else:
    print('You got {} wrong.  Keep trying!\n'.format(num_wrong))
print(output_frame.to_string(index=False))
```

<img src = 'xor.png'>

### Quiz: Build an XOR Multi-Layer Perceptron
Now, let's build a multi-layer perceptron from the AND, NOT, and OR perceptrons to create XOR logic!

The neural network below contains 3 perceptrons, A, B, and C. The last one (AND) has been given for you. The input to the neural network is from the first node. The output comes out of the last node.

The multi-layer perceptron below calculates XOR. Each perceptron is a logic operation of AND, OR, and NOT. However, the perceptrons A, B, and C don't indicate their operation. In the following quiz, set the correct operations for the perceptrons to calculate XOR.

<img src = 'build_xor.png'>

<img src = 'build_xor2.png'>

## Perceptron Algorithm

<img src = 'perc_algo.png'>
### Coding the Perceptron Algorithm
Time to code! In this quiz, you'll have the chance to implement the perceptron algorithm to separate the following data (given in the file data.csv).

<img src = 'data.png'>

Recall that the perceptron step works as follows. For a point with coordinates (p,q), label y, and prediction given by the equation $\hat{y} = step(w_1x_1 + w_2x_2 + b)$


1. If the point is correctly classified, do nothing.
2. If the point is classified positive, but it has a negative label, subtract $\alpha p$, $\alpha q$, and $\alpha$ from w_1, w_2, and b respectively.
3. If the point is classified negative, but it has a positive label, add $\alpha p$, $\alpha q$, and $\alpha$ to w_1, w_2,w1 and b respectively.

Then click on test run to graph the solution that the perceptron algorithm gives you. It'll actually draw a set of dotted lines, that show how the algorithm approaches to the best solution, given by the black solid line.

Feel free to play with the parameters of the algorithm (number of epochs, learning rate, and even the randomizing of the initial parameters) to see how your initial conditions can affect the solution!

```{python percep_algo}
import numpy as np
import pandas as pd
# Setting the random seed, feel free to change it and see different solutions.
np.random.seed(42)

def stepFunction(t):
    if t >= 0:
        return 1
    return 0

def prediction(X, W, b):
    return stepFunction((np.matmul(X,W)+b)[0])

# TODO: Fill in the code below to implement the perceptron trick.
# The function should receive as inputs the data X, the labels y,
# the weights W (as an array), and the bias b,
# update the weights and bias W, b, according to the perceptron algorithm,
# and return W and b.
def perceptronStep(X, y, W, b, learn_rate = 0.01):
    # Fill in code
    for index, value in enumerate(X):
        y_pred = prediction(X[index], W, b)
        # if prediction = 0 when it is actually a 1
        if y[index] - y_pred == 1:
            # We have 2 features so 2 weights
            W[0] += learn_rate * X[index][0]
            W[1] += learn_rate * X[index][1]
            b += learn_rate
        # if prediction = 1 when it is actually 0
        elif y[index] - y_pred == -1:
            W[0] -= learn_rate * X[index][0]
            W[1] -= learn_rate * X[index][1]
            b -= learn_rate
    return W, b
    
# This function runs the perceptron algorithm repeatedly on the dataset,
# and returns a few of the boundary lines obtained in the iterations,
# for plotting purposes.
# Feel free to play with the learning rate and the num_epochs,
# and see your results plotted below.
def trainPerceptronAlgorithm(X, y, learn_rate = 0.01, num_epochs = 25):
    x_min, x_max = min(X.T[0]), max(X.T[0])
    y_min, y_max = min(X.T[1]), max(X.T[1])
    W = np.array(np.random.rand(2,1))
    b = np.random.rand(1)[0] + x_max
    # These are the solution lines that get plotted below.
    boundary_lines = []
    for i in range(num_epochs):
        # In each epoch, we apply the perceptron step.
        W, b = perceptronStep(X, y, W, b, learn_rate)
        boundary_lines.append((-W[0]/W[1], -b/W[1]))
    return boundary_lines

data = pd.read_csv('data.csv', header = None)
data.head()

X = data.iloc[:,0:2]
y = data.iloc[:,2]
X.iloc[0:2, 0:]
y.iloc[0:2]

# trainPerceptronAlgorithm(X, y)

## Soln:
# def perceptronStep(X, y, W, b, learn_rate = 0.01):
#     for i in range(len(X)):
#         y_hat = prediction(X[i],W,b)
#         if y[i]-y_hat == 1:
#             W[0] += X[i][0]*learn_rate
#             W[1] += X[i][1]*learn_rate
#             b += learn_rate
#         elif y[i]-y_hat == -1:
#             W[0] -= X[i][0]*learn_rate
#             W[1] -= X[i][1]*learn_rate
#             b -= learn_rate
#     return W, b

```


## Neural Network Architecture

Let's define the combination of two new perceptrons as $w1*0.4 + w2*0.6 + b$. Which of the following values for the weights and the bias would result in the final probability of the point to be 0.88?

```{python nn_arch}
import math
w1 = 2
w2 = 6
b = -2

y = w1*0.4 + w2 * 0.6 + b

prob = 1/(1 + math.exp(-y))
print(f'With input weights of {w1}, {w2} and bias {b}, the probability is {prob}.')

w1 = 3
w2 = 5
b = -2.2

y = w1*0.4 + w2 * 0.6 + b

prob = 1/(1 + math.exp(-y))
print(f'With input weights of {w1}, {w2} and bias {b}, the probability is {prob}.')
```

## Regularisation
Which gives a smaller error for the points (1,1 -> blue) and (-1,-1 -> red)?

1. x1 + x2
2. 10x1 +10x2

```{python regular}
# for point (1,1)
pt1_1 = 1/(1 + math.exp(-2))
pt_neg1_neg1 = 1/(1 + math.exp(2))

print(f'With input weights of 1, 1 and bias 0, the probability is {pt1_1} and {pt_neg1_neg1}.')

pt1_1 = 1/(1 + math.exp(-20))
pt_neg1_neg1 = 1/(1 + math.exp(20))

print(f'With input weights of 10, 10 and bias 0, the probability is {pt1_1} and {pt_neg1_neg1}.')
```

Clearly the second algorithm classifies the points with more accuracy such that we can identify pt (1,1) as blue, and pt (-1, -1) as red (since the probability of 0.99999 > 0.88). But the model 10x1 + 10x2 is overfitting.

