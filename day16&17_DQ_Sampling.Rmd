---
title: "Day13"
author: "Data Analytics"
date: "2020-01-02"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Sampling
In this course we'll go much deeper into the theory behind what we've already learned. Second, we'll learn new and more powerful statistical techniques and metrics like standard deviation, z-scores, confidence intervals, probability estimation, and hypothesis testing (including A/B testing).

In this first course, we begin with discussing the details around getting data for analysis, and continue with trying to understand the intricacies around how data is structured and measured. We'll then move on with learning techniques to organize and visualize relatively large amounts of data, which will make the process of finding patterns considerably less difficult.

Below is a diagram describing the workflow we'll be focusing on throughout this first course.

<img src = dq1.png>

In this first mission, our focus will be on the details around getting data for analysis. As usual, we'll work with a real world data set. Before we dive into the technical details and start playing with the data, we begin with getting a sense about what statistics is.

At this stage in our learning journey, a one-sentence definition of statistics would probably sound dull and be difficult to grasp. We'll avoid defining statistics that way, and we'll discuss instead what sort of problems can be solved with statistics. Understanding what challenges we can overcome using statistics should give us a good sense about what statistics is.

Imagine you're managing a small tech company with 7 employees. At the end of the year, you piece together some data about your employees, with the intention of understanding better the state of your company. The data you collected is straightforward, and you can quickly make a few conclusions just by using a bit of arithmetics and logic.

<img src = dq2.png>

But years have gone by, and your business has grown into a successful company with 231 employees. You still want to get insights from data, but now you have so much of it that analyzing it has become difficult and inconvenient. As you continue to scale your company, analyzing data slowly gears toward becoming an impossible task.

<img src = dq3.png>

This is an example of a problem we can solve with statistics. Using statistical techniques, we can organize, summarize, and visualize large amounts of data to find patterns that otherwise would remain hidden.

<img src = dq4.png>

More years have gone by, and now you run an international company with over 5,0000 employees. You've recently made a company-wide change which resulted in making the work of your employees more demanding. Now you want to determine whether the employees have been impacted negatively in any significant way. If this is true, then the change may backfire in the long run, and it'd be a good decision to revert the process while it's still possible.

You reach out to your data analyst and ask for her opinion. She says that she can do a survey to collect data, and answer your question. Surveying over 50000 employees would be time-consuming and expensive, so you're being told that 100 people or so will be enough to survey to get an answer to your question.

One week later, the analysis shows that people generally report they are less satisfied with their work compared to the last year (when the change hadn't been yet implemented). Also, the inability to balance work and personal life is the main reported cause of dissatisfaction. Your analyst also tells you that the decrease in satisfaction is significant, meaning that it's very unlikely to simply have happened by chance. Something must have caused the decrease, and that something is probably the major change you've done recently.

This sort of scenario is very common in practice. As a data analyst, you'll often need to use a small set of data to answer questions about a much larger set of data.

<img src = dq5.png>

We'll learn ourselves throughout the statistics courses how to use a small set of data to answer questions about a much larger set of data.

Now we begin with discussing the details around collecting data, which is what the data analyst in our story did when she surveyed employees.

The data analyst in our previous example tried to answer whether people in the company are less satisfied at work compared to the previous year. Her question was about all the people inside the company. Yet she only selected a small group to answer the question.

In statistics, the set of all individuals relevant to a particular statistical question is called a population. For our analyst's question, all the people inside the company were relevant. So the population in this case consisted from all the people in the company.

A smaller group selected from a population is called a sample. When we select a smaller group from a population we do sampling. In our example, the data analyst took a sample of approximately 100 people from a population of over 50000 people.

<img src = dq6.png>

Whether a set of data is a sample or a population depends on the question we're trying to answer. For our analyst's question, the population consisted of all the company members. But if we change the question, the same group of individuals can become a sample.

For instance, if we tried to find out whether people at international companies are satisfied at work, then our group formed by over 50000 employees would become a sample. There are a lot of international companies out there, and ours is just one of them. The population (the set of all individuals relevant to this question) is made up of all the people working in all the international companies.

<img src = dq7.png>

Populations do not necessarily consist of people. Behavioral scientists, for instance, often try to answer questions about populations of monkeys, rats or other lab animals. In a similar way, other people try to answer questions about countries, companies, vegetables, soils, pieces of equipment produced in a factory, etc.

The individual elements of a population or a sample go under many names. You'll often see the elements of a population referred to as individuals, units, events, observations. These are all used interchangeably and refer to the same thing: the individual parts of a population. When we use the term "population individuals", the population is not necessarily composed of people. "Individuals" here is a general term that could refer to people, needles, frogs, stars, etc.

In the case of a sample, you'll often see this terminology used interchangeably: sample unit, sample point, sample individual, or sample observation.

<img src = dq8.png>

### Instructions
Now it's our turn to play the data analyst. We collected data about the salary of all the individuals in the company working in IT roles. Based on this data, we want to answer a series of questions. Depending on the question, our data is either a sample or a population. Identify which is the case, and assign to the corresponding variable the string 'population' or 'sample'. Here are the questions we need to answer:

- What's the average salary of the individuals in our company working in IT roles? (Assign either 'population' or 'sample' to the variable question1.)
- What's the proportion of individuals in the IT department having salaries under $60000? (Assign either 'population' or 'sample' to the variable question2.)
- What's the minimum salary in the entire company? (Assign either 'population' or 'sample' to the variable question3.)
- What's the minimum salary in the IT department of our company? (Assign either 'population' or 'sample' to the variable question4.)
- What's the proportion of salaries under $20000 in the entire company? (Assign either 'population' or 'sample' to the variable question5.)

```{python}
question1 = 'population'
question2 = 'population'
question3 = 'sample'
question4 = 'population'
question5 = 'sample'
```

For every statistical question we want to answer, we should try to use the population. In practice, that's not always possible because the populations of interest usually vary from large to extremely large. Also, getting data is generally not an easy task, so small populations often pose problems too.

These problems can be solved by sampling from the population that interests us. Although not as good as working with the entire population, working with a sample is the next best thing we can do.

When we sample, the data we get might be more or less similar to the data in the population. For instance, let's say we know that the average salary in our company is $34500, and the proportion of women is 60%. We take two samples and find these results:

<img src = dq9.png>

As you can see, the metrics of the two samples are different than the metrics of the population. A sample is by definition an incomplete set of data for the question we're trying to answer. For this reason, there's almost always some difference between the metrics of a population and the metrics of a sample. This difference can be seen as an error, and because it's the result of sampling, it's called sampling error.

A metric specific to a population is called a parameter, while one specific to a sample is called a statistic. In our example above, the average salary of all the employees is a parameter because it's a metric that describes the entire population. The average salaries from our two samples are examples of statistics because they only describe the samples.

Another way to think of the concept of the sampling error is as the difference between a parameter and a statistic:

$$ sampling\_error = parameter - statistic $$

At this point in the mission, we'll move from the tech company example to working with a real world data set. Our first challenge will be to measure sampling error using this data set.

The data set is about basketball players in WNBA (Women's National Basketball Association), and contains general information about players, along with their metrics for the season 2016-2017. The data set was put together by Thomas De Jonghe, and can be downloaded from Kaggle, where you can also find useful documentation for the data set.

#### Context
Scraped and copied from http://www.wnba.com/stats/player-stats/#?Season=2017&SeasonType=Regular%20Season&PerMode=Totals + http://www.wnba.com/ in general for the bio data.

#### Content
Stats from all games of season 2016-2017

+ G    = Games Played
+ MIN  = Minutes Played
+ FGM  = Field Goals Made
+ FGA  = Field Goals Attempts
+ FG%  = Field Goals %
+ 3PM  = 3Points Made
+ 3PA  = 3Points Attempts
+ 3P%  = 3Points %
+ FTM  = Free Throws made
+ FTA  = Free Throws Attempts
+ FT%  = Free Throws %
+ OREB = Offensive Rebounds
+ DREB = Defensive Rebounds
+ REB  = Total Rebounds
+ AST  = Assists
+ STL  = Steals
+ BLK  = Blocks
+ TO   = Turnovers
+ PTS  = Total points
+ DD2  = Double doubles
+ TD3  = Triple doubles


#### Inspiration
Compare WNBA to NBA in best players, average heights, ...

### Instructions
Get familiar with the data set.

1. Print the first five rows using DataFrame.head() and the last five rows with DataFrame.tail().
1. Find the number of rows and columns using DataFrame.shape.
1. Learn about each column from the documentation. You can also find useful documentation in this glossary and on WNBA's official page.


Take one measure of the sampling error.
1. Use the Games Played column to find the maximum number of games played by a player in the season 2016-2017. The data set contains all the players that had at least one game, so it's a population relative to our question. Find this parameter, and assign the result to a variable named parameter.
1. Using the Series.sample() method, sample randomly 30 players from the population, and assign the result to a variable named sample.
1. When calling Series.sample(), use the the argument random_state = 1. This makes your results reproducible and helps us with the answer checking (we'll discuss more about this in the next screen).
1. Find the maximum number of games using the sample, and assign the result to a variable named statistic.
1. Measure the sampling error, and assign the result to a variable named sampling_error.


```{python}
import pandas as pd
wnba = pd.read_csv('wnba.csv', header = 0)
print(wnba.head())
print(wnba.tail())
print(wnba.shape)
parameter = wnba['Games Played'].max()
sample = wnba.sample(30, random_state = 1)
statistic = sample['Games Played'].max()
sampling_error = parameter - statistic
print(f'The parameter of the population (i.e. max games played in the population) is {parameter}, \
while the statistic of the sample (i.e. max games played in the sample) is {statistic}. \
This results in a sampling error of {sampling_error}')
```

When we sample we want to minimize the sampling error as much as possible. We want our sample to mirror the population as closely as possible.

If we sampled to measure the mean height of adults in the US, we'd like our sample statistic (sample mean height) to get as close as possible to the population's parameter (population mean height). For this to happen, we need the individuals in our sample to form a group that is similar in structure with the group forming the population.

The US adult population is diverse, made of people of various heights. If we sampled 100 individuals from various basketball teams, then we'd almost certainly get a sample whose structure is significantly different than that of the population. As a consequence, we should expect a large sampling error (a large discrepancy between our sample's statistic (sample mean height) and the population's parameter (population mean height)).

In statistical terms, we want our samples to be representative of their corresponding populations. If a sample is representative, then the sampling error is low. The more representative a sample is, the smaller the sampling error. The less representative a sample is, the greater the sampling error.

<img src = dq10.png>

To make our samples representative, we can try to give every individual in the population an equal chance to be selected in our samples. We want a very tall individual to have the same chance as being selected as an individual having a medium or short height. To give every individual an equal chance of being picked, we need to sample randomly.

One way to perform random sampling is to generate random numbers and use them to select a few sample units from the population. In statistics, this sampling method is called simple random sampling, and it's often abbreviated as SRS.

In our previous exercise, we used Series.sample() to sample. This method performs simple random sampling by generating an array of random numbers, and then using those numbers to select values from a Series at the indices corresponding to those random numbers. The method can be also extended for DataFrame objects, where random rows or columns can be sampled.

When we use the random_state parameter, like we did in the previous exercise with Series.sample(30, random_state = 1), we make the generation of random numbers predictable. This is because Series.sample() uses a pseudorandom number generator under the hood. A pseudorandom number generator uses an initial value to generate a sequence of numbers that has properties similar to those of a sequence that is truly random. With random_state we specify that initial value used by the pseudorandom number generator.

If we want to generate a sequence of five numbers using a pseudorandom generator, and begin from an initial value of 1, we'll get the same five numbers no matter how many times we run the code. If we ran wnba['Games Played'].sample(5, random_state = 1) we'd get the same sample every time we run the code.

Pseudorandom number generators are of great use in scientific research where reproducible work is necessary. In our case, pseudorandom number generators allow us to work with the same samples as you do in the exercises, which allows in turn for a meaningful answer checking.

### Instructions
Let's visualize the discrepancy between a parameter and its corresponding statistics in the case of simple random sampling.

+ Using simple random sampling, take 100 samples from our WNBA data set, and for each sample measure the average points scored by a player during the 2016-2017 season. For each of the 100 iterations of a for loop:


    + Sample 10 values from the PTS column.
    + Compute the mean of this sample made of 10 values from the PTS column, and append the result to a list.
    + To make your results reproducible, vary the random_state parameter of the sample() method with values between 0 and 99. For the first iteration of the for loop, random_state should equal 0, for the second iteration should equal 1, for the third should equal 2, and so on.



+ Display the discrepancy between the parameter of interest (the mean of the PTS column) and the statistics obtained in the previous step.


    + Using plt.scatter(), display all the 100 sample means using a scatter plot. For the x-axis, use integers from 1 to 100 to designate the sample number. Use the y-axis for the sample means.
    + Using plt.axhline(), draw a horizontal line that represents the average number of points in the population.

```{python}
# import pandas as pd
# import matplotlib.pyplot as plt
# 
# wnba = pd.read_csv('wnba.csv')
# 
# pts_list = []
# for i in range(100):
#     sample = wnba.sample(10, random_state = i)
#     pts_list.append(sample['PTS'].mean())
# 
# plt.scatter(range(1,101), pts_list)     
# # Below gets the sample mean
# #plt.axhline(sum(pts_list)/(len(pts_list)))
# plt.axhline(wnba['PTS'].mean()) # population mean ...

```

#### Chris Albon's Tutorial on 'Apply Operations To Groups In Pandas'
[Link](https://chrisalbon.com/python/data_wrangling/pandas_apply_operations_to_groups/)

```{python}
# import modules
import pandas as pd
# Create dataframe
raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'], 
        'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'], 
        'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'], 
        'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],
        'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70]}
df = pd.DataFrame(raw_data, columns = ['regiment', 'company', 'name', 'preTestScore', 'postTestScore'])
df
# Create a groupby variable that groups preTestScores by regiment
groupby_regiment = df['preTestScore'].groupby(df['regiment'])
# "This grouped variable is now a GroupBy object. It has not actually computed anything yet 
# except for some intermediate data about the group key df['key1']. The idea is that this object 
# has all of the information needed to then apply some operation to each of the groups." 
# -                                                             Python for Data Analysis
groupby_regiment
# View a grouping
# Use list() to show what a grouping looks like
list(df['preTestScore'].groupby(df['regiment']))
# Descriptive statistics by group
df['preTestScore'].groupby(df['regiment']).describe()
# Mean of each regiment's preTestScore
groupby_regiment.mean()
# Mean preTestScores grouped by regiment and company
df['preTestScore'].groupby([df['regiment'], df['company']]).mean()
# Mean preTestScores grouped by regiment and company without heirarchical indexing
df['preTestScore'].groupby([df['regiment'], df['company']]).mean().unstack()
# Group the entire dataframe by regiment and company
df.groupby(['regiment', 'company']).mean()
# Number of observations in each regiment and company
df.groupby(['regiment', 'company']).size()

# Iterate an operations over groups
# Group the dataframe by regiment, and for each regiment,
for name, group in df.groupby('regiment'): 
    # print the name of the regiment
    print(name)
    # print the data of that regiment
    print(group.mean())

# Group by columns
# Specifically in this case: group by the data types of the columns (i.e. axis=1) 
# and then use list() to view what that grouping looks like 
list(df.groupby(df.dtypes, axis=1))

# In the dataframe "df", group by "regiments, take the mean values of the 
# other variables for those groups, then display them with the prefix_mean
df.groupby('regiment').mean().add_prefix('mean_')

# Create a function to get the stats of a group
def get_stats(group):
    return {'min': group.min(), 'max': group.max(), 'count': group.count(), 'mean': group.mean()}

# Create bins and bin up postTestScore by those pins    
bins = [0, 25, 50, 75, 100]
group_names = ['Low', 'Okay', 'Good', 'Great']
df['categories'] = pd.cut(df['postTestScore'], bins, labels=group_names)    

# Apply the get_stats() function to each postTestScore bin
df['postTestScore'].groupby(df['categories']).apply(get_stats).unstack()
```

From the scatter plot in the last screen, we can notice that the sample means vary a lot around the population mean. With a minimum sample mean of 115 points, a maximum of 301.4, and a population mean of roughly 201.8, we can tell that the sampling error is quite large for some of the cases.

<img src = dq12.png>

Because sample means vary a lot around the population mean, there's a good chance we get a sample that is not representative of the population:

<img src = dq13.png>

This problem can be solved by increasing the sample size. As we increase the sample size, the sample means vary less around the population mean, and the chances of getting an unrepresentative sample decrease.

In our last exercise we took 100 samples, and each had a sample size of 10 units. This is what happens when we repeat the procedure, but increase the size of the samples:

<img src = dq14.png>

We can easily see how sample means tend to vary less and less around the population mean as we increase the sample size. From this observation we can make two conclusions:

- Simple random sampling is not a reliable sampling method when the sample size is small. Because sample means vary a lot around the population mean, there's a good chance we'll get an unrepresentative sample.
- When we do simple random sampling, we should try to get a sample that is as large as possible. A large sample decreases the variability of the sampling process, which in turn decreases the chances that we'll get an unrepresentative sample.

<img src = dq15.png>

Because simple random sampling is entirely random, it can leave out certain population individuals that are of great interest to some of the questions we may have.

For example, players in basketball play in different positions on the court. The metrics of a player (number of points, number of assists, etc.) depend on their position, and we might want to analyze the patterns for each individual position. If we perform simple random sampling, there's a chance that some categories won't be included in our sample. In other words, it's not guaranteed that we'll have a representative sample that has observations for every position we want to analyze.

There are five unique positions in our data set:

> print(wnba['Pos'].unique()) <br>
> array(['F', 'G/F', 'G', 'C', 'F/C'], dtype=object)


Let's decipher quickly each abbreviation:

<img src = dq16.png>

The downside of simple random sampling is that it can leave out individuals playing in a certain position on the field. Visually, and on a smaller scale, this is what could happen:

<img src = dq17.png>

To ensure we end up with a sample that has observations for all the categories of interest, we can change the sampling method. We can organize our data set into different groups, and then do simple random sampling for every group. We can group our data set by player position, and then sample randomly from each group.

Visually, and on a smaller scale, we need to do this:

<img src = dq18.png>

This sampling method is called stratified sampling, and each stratified group is also known as a stratum.

### Instructions
Perform stratified sampling: stratify the data set by player position, and then do simple random sampling on every stratum. At the end, use the sample to find which position has the greatest number of points per game.

+ Create a new column which describes the number of points a player scored per game during the season. The number of total points a player scored the entire season is stored in the PTS column, and the number of games played in the Games Played column. Give the new column a relevant name.
+ Stratify the wnba data set by player position. The Pos column describes a player's position on the field. Assign each stratum to a different variable.
+ Loop through the strata, and for each stratum:

    + Sample 10 observations using simple random sampling (set random_state = 0).
    + Find the mean points per game using the sample. Use the new column you've created earlier.
    + Find a way to store the mean along with its corresponding position. You can use a dictionary.
    
+ Find the position that has the greatest number of points per game, and assign its name to a variable named position_most_points.

    + To find the dictionary key that has the greatest dictionary value, you can use this technique.




```{python}
print(wnba['Pos'].unique())
wnba['Pts_per_game'] = wnba['PTS']/wnba['Games Played']
wnba_grouped = wnba['Pts_per_game'].groupby(wnba['Pos'])

wnba['Pts_per_game'].groupby(wnba['Pos']).describe()
```





